{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gender detector...\n",
      "Loading face detector...\n",
      "Loading emotion detector...\n",
      "Loading known faces...\n",
      "5 faces loaded\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "a pressed\n",
      "*** Output predictions selected *** \n",
      "Audio file ../lang/es/speech/no_image.mp3 loaded!\n",
      "re-encoding\n",
      "a pressed\n",
      "*** Output predictions selected *** \n",
      "[{'NAME': 'Josu', 'EMOTION': 'happy', 'GENDER': 'man', 'FULL_NAME': 'Josu-AbHMzdxQ'}]\n",
      "Audio file ../audio/known/Josu-AbHMzdxQ.mp3 loaded!\n",
      "Audio file ../lang/es/emotion/man/happy.mp3 loaded!\n",
      "l pressed\n",
      "*** Language change *** \n",
      "Audio file ../lang/en/speech/lang_change.mp3 loaded!\n",
      "re-encoding\n",
      "a pressed\n",
      "*** Output predictions selected *** \n",
      "[{'NAME': 'Josu', 'EMOTION': 'happy', 'GENDER': 'man', 'FULL_NAME': 'Josu-AbHMzdxQ'}]\n",
      "Audio file ../lang/en/emotion/happy.mp3 loaded!\n",
      "Audio file ../audio/known/Josu-AbHMzdxQ.mp3 loaded!\n",
      "re-encoding\n",
      "q pressed\n"
     ]
    }
   ],
   "source": [
    "from speech_utils import *\n",
    "from audio_utils import *\n",
    "from text_utils import *\n",
    "from lang_utils import *\n",
    "from model_utils import *\n",
    "import cv2\n",
    "from operator import itemgetter\n",
    "\n",
    "# Variables\n",
    "ENCODING_FREQ = 10\n",
    "encoding_count = 0\n",
    "last_faces_count = 0\n",
    "face_encodings = []\n",
    "predictions = []\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "emotion_label_window = []\n",
    "gender_label_window = []\n",
    "last_faces = []\n",
    "label_dict = {'EMOTION': '', 'GENDER': '', 'NAME': '', 'FULL_NAME': ''}\n",
    "\n",
    "# Language and localization\n",
    "AVAILABLE_LANGUAGES = ['es','en']\n",
    "LANGUAGE = 'es'\n",
    "LANGUAGE_PATH = '../lang/'\n",
    "AUDIO_PATH = '../audio/'\n",
    "IMAGE_PATH = '../images/'\n",
    "\n",
    "lang_helper = Lang_Helper(AVAILABLE_LANGUAGES, LANGUAGE_PATH, AUDIO_PATH, IMAGE_PATH, LANGUAGE)\n",
    "\n",
    "\n",
    "# Models\n",
    "model_helper = Model_Helper('../models/face/haarcascade_frontalface_default.xml', \n",
    "                            '../models/emotion/simple_CNN.530-0.65.hdf5', \n",
    "                            '../models/gender/simple_CNN.81-0.96.hdf5',\n",
    "                            AUDIO_PATH, IMAGE_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# Input image \n",
    "cv2.namedWindow('main')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    predictions = []\n",
    "    encoding_count += 1\n",
    "    last_faces_count = len(last_faces)\n",
    "    last_faces = []\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = model_helper.face_detection.detectMultiScale(gray, 1.3, 5)\n",
    "      \n",
    "    do_encode = encoding_count>=ENCODING_FREQ or last_faces_count!=len(faces) \n",
    "    \n",
    "    if (do_encode):\n",
    "        face_encodings = []\n",
    "    \n",
    "    face_index = 0\n",
    "    \n",
    "    for (x,y,w,h) in sorted(faces, key=itemgetter(0)):\n",
    "                \n",
    "        pred_dict = label_dict.copy();\n",
    "        \n",
    "        face_index +=1 \n",
    "        face = frame[(y - y_offset):(y + h + y_offset),\n",
    "                    (x - x_offset):(x + w + x_offset)]        \n",
    "        if (do_encode):\n",
    "            print('re-encoding')\n",
    "            face_encodings.append(face_recognition.face_encodings(frame, [tuple([int(y), int(x+w), int(y+h), int(x)])])[0])\n",
    "            encoding_count = 0\n",
    "        \n",
    "        try:\n",
    "            if (len(face_encodings)>0 & face_index -1 < len(face_encodings)):\n",
    "                for i in range(len(model_helper.known_faces)):\n",
    "                    match = face_recognition.compare_faces([model_helper.known_faces[i][2]], face_encodings[face_index-1])\n",
    "                    if match[0]:\n",
    "                        pred_dict['NAME'] = model_helper.known_faces[i][0]\n",
    "                        pred_dict['FULL_NAME'] = model_helper.known_faces[i][1]\n",
    "                        break;\n",
    "                  \n",
    "        except Exception as e: \n",
    "            print('*******')\n",
    "            print(e)\n",
    "            print('*******')\n",
    "            continue            \n",
    "        #print('-----')\n",
    "        last_faces.append(cv2.cvtColor(face.copy(), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        gray_face = gray[(y - y_offset_emotion):(y + h + y_offset_emotion),\n",
    "                        (x - x_offset_emotion):(x + w + x_offset_emotion)]\n",
    "        try:\n",
    "            face = cv2.resize(face, (48, 48))\n",
    "            gray_face = cv2.resize(gray_face, (48, 48))            \n",
    "        except:\n",
    "            continue\n",
    "        face = np.expand_dims(face, 0)\n",
    "        face = preprocess_input(face)\n",
    "        gender_label_arg = np.argmax(model_helper.gender_classifier.predict(face))\n",
    "        gender = gender_labels[gender_label_arg]\n",
    "        gender_label_window.append(gender)\n",
    "\n",
    "        gray_face = preprocess_input(gray_face)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_label_arg = np.argmax(model_helper.emotion_classifier.predict(gray_face))\n",
    "        emotion = emotion_labels[emotion_label_arg]\n",
    "        emotion_label_window.append(emotion)\n",
    "\n",
    "        if len(gender_label_window) >= frame_window:\n",
    "            emotion_label_window.pop(0)\n",
    "            gender_label_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_label_window)\n",
    "            gender_mode = mode(gender_label_window)\n",
    "        except:\n",
    "            continue\n",
    "        if gender_mode == gender_labels[0]:\n",
    "            gender_color = (255, 0, 0)\n",
    "        else:\n",
    "            gender_color = (0, 255, 0)   \n",
    "        \n",
    "        pred_dict['EMOTION'] = emotion_mode\n",
    "        pred_dict['GENDER'] = gender_mode\n",
    "        \n",
    "        display_text = lang_helper.get_formatted_language_text(pred_dict)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), gender_color, 2)\n",
    "        cv2.putText(frame, display_text, (x, y - 30), font,\n",
    "                        .7, gender_color, 1, cv2.LINE_AA)\n",
    "        \n",
    "        predictions.append(pred_dict)\n",
    "\n",
    "    try:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('main', frame)        \n",
    "    except:\n",
    "        continue\n",
    "    c = chr(cv2.waitKey(2)& 255)\n",
    "    if (c!= 'Ã¿'):\n",
    "        print(c + \" pressed\")   \n",
    "    command = lang_helper.get_command(c.upper())\n",
    "    if (command == 'language'):\n",
    "        print('*** Language change *** ')\n",
    "        lang_helper.switch_to_next_language()\n",
    "        lang_helper.talk('lang_change')\n",
    "    elif (command == 'who'):\n",
    "        print('*** Output predictions selected *** ')\n",
    "        if (len(predictions) > 0):\n",
    "            lang_audios =  lang_helper.get_formatted_language_audios(predictions)\n",
    "            for lang_audio in lang_audios:\n",
    "                lang_helper.play(lang_audio)\n",
    "        else:\n",
    "            lang_helper.talk('no_image')            \n",
    "    elif (command == 'save'):\n",
    "        print('*** Save person selected *** ')\n",
    "        try:\n",
    "            if (len(last_faces)==1):\n",
    "                name = '##NONE##'\n",
    "                while name == '##NONE##':\n",
    "                    lang_helper.talk('who')\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    else:\n",
    "                        name = lang_helper.capture_name() \n",
    "                        if (name=='##NONE##'):\n",
    "                            lang_helper.talk('not_understand')                            \n",
    "                        elif (name == 'cancel'):\n",
    "                            lang_helper.talk('canceled')                            \n",
    "                            break\n",
    "                        else:\n",
    "                            print('saving face...')\n",
    "                            full_name = model_helper.save_face(name, lang_helper.current_language, last_faces[0], face_encodings[face_index-1])    \n",
    "                            print('///////')\n",
    "                            print(full_name)\n",
    "                            print(lang_helper.audio_path + 'known/' + full_name + '.mp3')\n",
    "                            if (full_name!=''):\n",
    "                                lang_helper.play(lang_helper.audio_path + 'known/' + full_name + '.mp3')\n",
    "                                lang_helper.talk('saved')                        \n",
    "                            break                \n",
    "            elif (len(last_faces)>1):\n",
    "                lang_helper.talk('more_than_one_face')\n",
    "            else:\n",
    "                lang_helper.talk('no_image')\n",
    "        except:\n",
    "            continue\n",
    "    elif (command == 'quit'):\n",
    "        break\n",
    "    \n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
