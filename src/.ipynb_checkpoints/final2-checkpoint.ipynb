{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading gender detector...\n",
      "Loading face detector...\n",
      "Loading emotion detector...\n",
      "Loading known faces...\n",
      "3 faces loaded\n"
     ]
    }
   ],
   "source": [
    "import speech_recognition as sr\n",
    "from google.cloud import speech\n",
    "import io\n",
    "import os\n",
    "\n",
    "#######################\n",
    "GOOGLE_CLOUD_SPEECH_CREDENTIALS_PATH = '../files/TFM project-287dc6d9869a.json'\n",
    "#######################\n",
    "\n",
    "def transcript_audio(filepath, language, use_cloud):\n",
    "    transcript = '##NONE##'\n",
    "    # The name of the audio file to transcribe\n",
    "    file_name = os.path.join(os.path.dirname(''), filepath)\n",
    "    \n",
    "    if use_cloud:\n",
    "        try:\n",
    "             # Instantiates a client\n",
    "            speech_client = speech.Client.from_service_account_json(GOOGLE_CLOUD_SPEECH_CREDENTIALS_PATH)\n",
    "            \n",
    "            # Loads the audio into memory\n",
    "            with io.open(file_name, 'rb') as audio_file:\n",
    "                content = audio_file.read()\n",
    "                sample = speech_client.sample(\n",
    "                    content,\n",
    "                    source_uri=None,\n",
    "                    encoding='LINEAR16',\n",
    "                    sample_rate_hertz=16000)\n",
    "\n",
    "            # Detects speech in the audio file\n",
    "            alternatives = sample.recognize(language)\n",
    "            \n",
    "            if (len(alternatives)>0):\n",
    "                transcript = alternatives[0].transcript\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            \n",
    "    if (transcript == '##NONE##'):\n",
    "        try:\n",
    "            r = sr.Recognizer()\n",
    "            with sr.AudioFile(file_name) as source:\n",
    "                audio = r.record(source) \n",
    "            # for testing purposes, we're just using the default API key\n",
    "            # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\", show_all=True)`\n",
    "            # instead of `r.recognize_google(audio, show_all=True)`\n",
    "            alternatives = r.recognize_google(audio, show_all=False)\n",
    "            if (len(alternatives)>0):\n",
    "                transcript = alternatives\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "       \n",
    "    return transcript\n",
    "\n",
    "\n",
    "# Audio Play\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import sys\n",
    "import pygame as pg\n",
    "\n",
    "def play_audio(audio_file, volume=0.8):\n",
    "    '''\n",
    "    stream music with mixer.music module in a blocking manner\n",
    "    this will stream the sound from disk while playing\n",
    "    '''\n",
    "    # set up the mixer\n",
    "    freq = 44100     # audio CD quality\n",
    "    bitsize = -16    # unsigned 16 bit\n",
    "    channels = 2     # 1 is mono, 2 is stereo\n",
    "    buffer = 2048    # number of samples (experiment to get best sound)\n",
    "    pg.mixer.init()\n",
    "    # volume value 0.0 to 1.0\n",
    "    pg.mixer.music.set_volume(volume)\n",
    "    clock = pg.time.Clock()\n",
    "    try:\n",
    "        pg.mixer.music.load(audio_file)\n",
    "        print(\"Audio file {} loaded!\".format(audio_file))\n",
    "    except pg.error:\n",
    "        print(\"File {} not found! ({})\".format(audio_file, pg.get_error()))\n",
    "        return\n",
    "    pg.mixer.music.play()\n",
    "    while pg.mixer.music.get_busy():\n",
    "        # check if playback has finished\n",
    "        clock.tick(30)\n",
    "        \n",
    "def play_any_audio(filename):\n",
    "    pg.mixer.init()\n",
    "    pg.mixer.music.load(filename)\n",
    "    pg.mixer.music.play()\n",
    "\n",
    "def play_wav_audio(filename):    \n",
    "    WAVE_FILENAME = filename\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Plays a wave file.\\n\\nUsage: %s filename.wav\" % WAVE_FILENAME)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    wf = wave.open(WAVE_FILENAME, 'rb')\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        data = wf.readframes(frame_count)\n",
    "        return (data, pyaudio.paContinue)\n",
    "\n",
    "    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                    channels=wf.getnchannels(),\n",
    "                    rate=wf.getframerate(),\n",
    "                    output=True,\n",
    "                    stream_callback=callback)\n",
    "\n",
    "    stream.start_stream()\n",
    "\n",
    "    while stream.is_active():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    wf.close()\n",
    "\n",
    "    p.terminate()\n",
    "    \n",
    "def record_audio(filename, seconds): \n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    #CHANNELS = 2\n",
    "    CHANNELS = 1\n",
    "    #RATE = 44100\n",
    "    RATE = 16000\n",
    "\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * seconds)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()\n",
    "\n",
    "import gettext\n",
    "\n",
    "class Text_Helper:    \n",
    "    def __init__(self, language_path, audio_path, current_language):\n",
    "        self.language_path = language_path\n",
    "        self.audio_path = audio_path\n",
    "        self.current_language = current_language   \n",
    "\n",
    "        \n",
    "    def get_():   \n",
    "        l = gettext.translation('text_', localedir=self.language_path, languages=[self.current_language])\n",
    "        l.install()\n",
    "        _ = l.gettext\n",
    "        return _\n",
    "\n",
    "\n",
    "    \n",
    "from gtts import gTTS\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "import gettext\n",
    "from audio_utils import *\n",
    "import cv2\n",
    "from random import *\n",
    "from speech_utils import *\n",
    "import string\n",
    "\n",
    "class Lang_Helper:\n",
    "\n",
    "\n",
    "     \n",
    "    def __init__(self, available_languages, language_path, audio_path, image_path, current_language):\n",
    "        self.available_languages = available_languages\n",
    "        self.language_path = language_path\n",
    "        self.audio_path = audio_path\n",
    "        self.audio_path = audio_path\n",
    "        self.current_language = current_language   \n",
    "        self.config_audios = []\n",
    "        with open(self.language_path + self.current_language + '/audio_config.txt') as f:\n",
    "            for line in f:\n",
    "                self.config_audios.append(line.rstrip())\n",
    "        l = gettext.translation('text_', localedir=self.language_path, languages=[self.current_language])\n",
    "        l.install()\n",
    "        self._ = l.gettext  \n",
    "\n",
    "    def change_language(self, new_lang):\n",
    "        self.config_audios = []\n",
    "        with open(self.language_path+new_lang+'/audio_config.txt') as f:\n",
    "            for line in f:\n",
    "                self.config_audios.append(line.rstrip())\n",
    "        \n",
    "        l = gettext.translation('text_', localedir=self.language_path, languages=[new_lang])\n",
    "        l.install()\n",
    "        self._ = l.gettext  \n",
    "        self.current_language = new_lang        \n",
    "                \n",
    "    def switch_to_next_language(self):\n",
    "        idx = self.available_languages.index(self.current_language)\n",
    "        new_idx = 0\n",
    "        if (idx+1 < len(self.available_languages)):\n",
    "            new_idx = idx+1\n",
    "        self.change_language(self.available_languages[new_idx])\n",
    "        \n",
    "\n",
    "    def get_language_commands(self, available_commands):\n",
    "        l = gettext.translation('text_', localedir=self.language_path, languages=[self.current_language])\n",
    "        l.install()\n",
    "        _ = l.gettext\n",
    "        commands = []\n",
    "        for command in available_commands:\n",
    "            commands.append(_(command))\n",
    "        return commands\n",
    "\n",
    "    def capture_speech(self):\n",
    "        rand = \"\".join(choice(string.ascii_letters) for x in range(randint(8, 8)))\n",
    "        temp_wav = self.audio_path + 'temp/' + rand + '.wav'\n",
    "        \n",
    "        #Play beep\n",
    "        self.play(self.audio_path + 'beep.mp3')\n",
    "        \n",
    "        #Record audio\n",
    "        record_audio(temp_wav, 2)\n",
    "        self.talk('one_moment')\n",
    "        \n",
    "        #Transcript audio\n",
    "        transcript = transcript_audio(temp_wav, self.current_language, True) \n",
    "        transcript = unidecode(transcript)\n",
    "        print('***'+transcript+'***')\n",
    "        os.remove(temp_wav)\n",
    "        return transcript.strip()\n",
    "\n",
    "    def capture_selected_command(self):\n",
    "        available_commands = ['who','what','save','language','cancel','repeat', 'quit','options', 'keys', 'repeat']\n",
    "        lang_commands = self.get_language_commands(available_commands)\n",
    "        transcript = self.capture_speech()\n",
    "        if (transcript == '' or transcript == '##NONE##'): \n",
    "            return '##NONE##'\n",
    "        elif (transcript.lower() in lang_commands):\n",
    "            try:\n",
    "                return available_commands[lang_commands.index(transcript.lower())]\n",
    "            except Exception as e: \n",
    "                print('**c****')\n",
    "                print(e)\n",
    "                print('**c****')\n",
    "                return '##NONE##'\n",
    "            return transcript.lower()\n",
    "        else:\n",
    "            #return '##UNKNOWN##'\n",
    "            return '##NONE##'\n",
    "\n",
    "    def capture_custom_command(self, available_commands):\n",
    "        lang_commands = self.get_language_commands(available_commands)\n",
    "        transcript = self.capture_speech()\n",
    "        if (transcript == '' or transcript == '##NONE##'): \n",
    "            return '##NONE##'\n",
    "        elif (transcript.lower() in lang_commands):\n",
    "            try:\n",
    "                return available_commands[lang_commands.index(transcript.lower())]\n",
    "            except Exception as e: \n",
    "                print('**c****')\n",
    "                print(e)\n",
    "                print('**c****')\n",
    "                return '##NONE##'\n",
    "            return transcript.lower()\n",
    "        else:\n",
    "            #return '##UNKNOWN##'\n",
    "            return '##NONE##'\n",
    "        \n",
    "\n",
    "    def capture_name(self):\n",
    "        available_commands = ['cancel']\n",
    "        lang_commands = self.get_language_commands(available_commands)        \n",
    "        \n",
    "        transcript = self.capture_speech()    \n",
    "        transcript = unidecode(transcript)\n",
    "        \n",
    "        #if transcript didn't capture anything then exit \n",
    "        if (transcript == '' or transcript == '##NONE##'): \n",
    "            return '##NONE##'\n",
    "        #if transcript captures cancelation then cancel\n",
    "        elif (transcript.lower() in lang_commands):                \n",
    "            return available_commands[lang_commands.index(transcript.lower())]\n",
    "        #if transcript ok then proceed\n",
    "        else:\n",
    "            return transcript\n",
    "            \n",
    "        \n",
    "    def get_command(self, c):\n",
    "        command = '##NONE##'\n",
    "        if (c==' '):\n",
    "            try:           \n",
    "                while command == '##NONE##':\n",
    "                    self.talk('choose_short')\n",
    "                    nc = chr(cv2.waitKey(2)& 255)\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    else:\n",
    "                        command = self.capture_selected_command()\n",
    "                        if (command=='##NONE##'):\n",
    "                            self.talk('not_understand')\n",
    "                            self.talk('repeat_options')\n",
    "                            new_command = '##NONE##'\n",
    "                            new_command = self.capture_custom_command(['yes', 'no'])\n",
    "                            if (new_command == 'yes'):\n",
    "                                self.talk('commands')\n",
    "                            else:\n",
    "                                self.talk('ok')\n",
    "                                command = 'cancel'\n",
    "                                break;\n",
    "                        else:                        \n",
    "                            break                \n",
    "            except Exception as e: \n",
    "                print('*c*****')\n",
    "                print(e)\n",
    "                print('*c*****')\n",
    "        elif (c=='0'):\n",
    "            try:           \n",
    "                while command == '##NONE##':\n",
    "                    self.talk('choose')\n",
    "                    nc = chr(cv2.waitKey(2)& 255)\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    else:\n",
    "                        new_command = '##NONE##'\n",
    "                        new_command = self.capture_custom_command((['commands', 'keys', 'cancel']))\n",
    "                        if (new_command=='##NONE##'):\n",
    "                            self.talk('not_understand')\n",
    "                            self.talk('commands')\n",
    "                            return self.get_command(' ')\n",
    "                        elif (new_command=='commands'):\n",
    "                            self.talk('commands')\n",
    "                            return self.get_command(' ')\n",
    "                        elif (new_command=='keys'):\n",
    "                            self.talk('keys')\n",
    "                            return self.get_command(' ') \n",
    "                        elif (new_command=='cancel'):\n",
    "                            return 'cancel'                     \n",
    "                        else :                        \n",
    "                            self.talk('not_understand')\n",
    "                            self.talk('commands')\n",
    "                            return self.get_command(' ')\n",
    "            except Exception as e: \n",
    "                print('*c*****')\n",
    "                print(e)\n",
    "                print('*c*****')\n",
    "                \n",
    "        if (c=='L' or command=='language'):\n",
    "            return 'language'\n",
    "        elif (c=='A' or command=='who'):\n",
    "            return 'who' \n",
    "        elif  (c=='S' or command=='save'):\n",
    "            return 'save'\n",
    "        elif (c=='C' or command=='cancel'):\n",
    "            self.talk('canceled')\n",
    "            return 'cancel'\n",
    "        elif (command=='repeat'):\n",
    "            self.talk('commands')\n",
    "            return self.get_command(' ')        \n",
    "        if (c=='Q' or command=='quit'):\n",
    "            return 'quit'\n",
    "\n",
    "    def get_language_audios(path, audios, preds):\n",
    "        lang_audios = []\n",
    "        for audio in audios:\n",
    "            audio_path = path + audio\n",
    "            for pred in preds:\n",
    "                audio_path = audio_path.replace('['+pred+']', preds[pred])\n",
    "            lang_audios.append(audio_path)\n",
    "        return lang_audios\n",
    "\n",
    "    def get_formatted_language_audios(self, predictions):\n",
    "        lang_audios = []\n",
    "        try:\n",
    "            print(predictions)\n",
    "            for prediction in predictions:\n",
    "                for audio in self.config_audios:\n",
    "                    key = audio.split(':')[0]\n",
    "                    if (key == 'GENDER' and prediction['FULL_NAME'] != ''):\n",
    "                        audio_path = self.audio_path + 'known/' + prediction['FULL_NAME'] + '.mp3'\n",
    "                        lang_audios.append(audio_path)\n",
    "                    else:\n",
    "                        audio_path = self.language_path + self.current_language + '/' + audio.split(':')[1]                    \n",
    "                        for key in prediction:\n",
    "                            audio_path = audio_path.replace('['+key+']', prediction[key])\n",
    "                        lang_audios.append(audio_path)\n",
    "                    \n",
    "\n",
    "        except Exception as e: \n",
    "            print('*a******')\n",
    "            print(e)\n",
    "            print('*a******')\n",
    "        return lang_audios\n",
    "\n",
    "    def get_formatted_language_text(self, prediction):\n",
    "        lang_text = ''\n",
    "        try:\n",
    "            text_config = ''\n",
    "            with open(self.language_path + self.current_language + '/text_config.txt') as f:\n",
    "                for line in f:\n",
    "                    text_config += line.rstrip()\n",
    "            g = text_config.split(':')[0]\n",
    "            lang_text = text_config.split(':')[1]\n",
    "            \n",
    "            for key in prediction:\n",
    "                g = g.replace('['+key+']', prediction[key])\n",
    "                \n",
    "            l = gettext.translation('text_' + g, localedir=self.language_path, languages=[self.current_language])\n",
    "            l.install()\n",
    "            __ = l.gettext        \n",
    "            t = ''\n",
    "            if (prediction['NAME'] != ''):  \n",
    "                t = prediction['NAME']\n",
    "            else:\n",
    "                if(prediction['GENDER'] != ''):\n",
    "                    t = __(str(prediction['GENDER']))\n",
    "                    \n",
    "            lang_text = lang_text.replace('[GENDER]', t) \n",
    "            t = ''\n",
    "            if(prediction['EMOTION'] != ''):\n",
    "                t = __(prediction['EMOTION'])\n",
    "                \n",
    "            lang_text = lang_text.replace('[EMOTION]', t)      \n",
    "        except Exception as e: \n",
    "            print('*t******')\n",
    "            print(e)\n",
    "            print('*t******')\n",
    "        return lang_text\n",
    "\n",
    "    def talk(self, audio_name):\n",
    "        self.play(self.language_path + self.current_language + '/speech/' + audio_name + '.mp3')\n",
    "\n",
    "    def play(self, audio_path):\n",
    "        play_audio(audio_path)        \n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.misc import imread\n",
    "from scipy.misc import imresize\n",
    "from random import shuffle\n",
    "import numpy as np\n",
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "import glob\n",
    "import os\n",
    "import face_recognition\n",
    "import string \n",
    "from random import *\n",
    "from gtts import gTTS\n",
    "\n",
    "def get_labels(dataset_name):\n",
    "    if dataset_name == 'fer2013':\n",
    "        return {0:'angry',1:'disgust',2:'sad',3:'happy',\n",
    "                    4:'sad',5:'surprise',6:'neutral'}\n",
    "    elif dataset_name == 'imdb':\n",
    "        return {0:'woman', 1:'man'}\n",
    "    else:\n",
    "        raise Exception('Invalid dataset name')\n",
    "\n",
    "def preprocess_input(images):\n",
    "    images = images/255.0\n",
    "    return images\n",
    "\n",
    "def _imread(image_name):\n",
    "        return imread(image_name)\n",
    "\n",
    "def _imresize(image_array, size):\n",
    "        return imresize(image_array, size)\n",
    "\n",
    "def split_data(ground_truth_data, training_ratio=.8, do_shuffle=False):\n",
    "    ground_truth_keys = sorted(ground_truth_data.keys())\n",
    "    if do_shuffle == True:\n",
    "        shuffle(ground_truth_keys)\n",
    "    num_train = int(round(training_ratio * len(ground_truth_keys)))\n",
    "    train_keys = ground_truth_keys[:num_train]\n",
    "    validation_keys = ground_truth_keys[num_train:]\n",
    "    return train_keys, validation_keys\n",
    "\n",
    "def display_image(image_array):\n",
    "    image_array =  np.squeeze(image_array).astype('uint8')\n",
    "    plt.imshow(image_array)\n",
    "    plt.show()\n",
    "\n",
    "def to_categorical(integer_classes, num_classes=2):\n",
    "    integer_classes = np.asarray(integer_classes, dtype='int')\n",
    "    num_samples = integer_classes.shape[0]\n",
    "    categorical = np.zeros((num_samples, num_classes))\n",
    "    categorical[np.arange(num_samples), integer_classes] = 1\n",
    "    return categorical\n",
    "\n",
    "\n",
    "# parameters\n",
    "detection_model_path = '../models/face/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = '../models/emotion/simple_CNN.530-0.65.hdf5'\n",
    "gender_model_path = '../models/gender/simple_CNN.81-0.96.hdf5'\n",
    "emotion_labels = get_labels('fer2013')\n",
    "gender_labels = get_labels('imdb')\n",
    "frame_window = 10\n",
    "x_offset_emotion = 20\n",
    "y_offset_emotion = 40\n",
    "x_offset = 30\n",
    "y_offset = 60\n",
    "\n",
    "class Model_Helper:    \n",
    "    def __init__(self, detection_model_path, emotion_model_path, current_language, audio_path, image_path):\n",
    "        self.audio_path = audio_path\n",
    "        self.image_path = image_path\n",
    "\n",
    "        print('Loading gender detector...')\n",
    "        self.gender_classifier = load_model(gender_model_path)\n",
    "        \n",
    "        print('Loading face detector...')\n",
    "        self.face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "        \n",
    "        print('Loading emotion detector...')\n",
    "        self.emotion_classifier = load_model(emotion_model_path)  \n",
    "\n",
    "        print('Loading known faces...')\n",
    "\n",
    "        self.known_faces = []\n",
    "\n",
    "        for filepath in glob.iglob(self.image_path + 'known/*.*', recursive=True):  \n",
    "            try:\n",
    "                filename = os.path.splitext(os.path.basename(filepath))[0]\n",
    "                name = os.path.splitext(filename)[0].split('-')[0]\n",
    "                picture = face_recognition.load_image_file(filepath)\n",
    "                encoding = face_recognition.face_encodings(picture)[0]\n",
    "                self.known_faces.append([name, filename, encoding])\n",
    "            except Exception as e: \n",
    "                try:\n",
    "                    os.remove(self.image_path + 'known/' + filename+'.jpg')\n",
    "                    os.remove(self.audio_path + 'known/' + filename+'.mp3')\n",
    "                except Exception as e: \n",
    "                    print(e)                    \n",
    "            \n",
    "\n",
    "        print(str(len(self.known_faces)) + ' faces loaded')\n",
    "\n",
    "    def update_known_faces(self, name, audio_file_name, face_encoding, current_encoding):\n",
    "        temp_faces = []\n",
    "        \n",
    "        # Remove previous faces with same encoding\n",
    "        for i in range(len(self.known_faces)):\n",
    "            match = face_recognition.compare_faces([self.known_faces[i][2]], current_encoding)\n",
    "            if match[0]:\n",
    "                print(self.known_faces[i][1] + ' is match')\n",
    "                image_file = self.image_path + 'known/' + self.known_faces[i][1]+'.jpg'\n",
    "                audio_file = self.audio_path + 'known/' + self.known_faces[i][1]+'.mp3'\n",
    "                os.remove(image_file)\n",
    "                print(image_file + ' deleted')\n",
    "                os.remove(audio_file)\n",
    "                print(audio_file + ' deleted')\n",
    "            else:\n",
    "                print(self.known_faces[i][1] + ' no match')\n",
    "                temp_faces.append(self.known_faces[i])\n",
    "        # Add new encoding and data to known faces\n",
    "        temp_faces.append([name, audio_file_name, face_encoding])     \n",
    "        print(name + ' added')\n",
    "        self.known_faces = temp_faces      \n",
    "\n",
    "    def save_face(self, name, language, face, current_encoding):\n",
    "        try:\n",
    "            rand = \"\".join(choice(string.ascii_letters) for x in range(randint(8, 8)))\n",
    "            full_name = name + '-' + rand\n",
    "            path_audio = self.audio_path + 'known/' + full_name + '.mp3'\n",
    "            path_image = self.image_path + 'known/' + full_name + '.jpg'\n",
    "            \n",
    "            #Convert transcript to standard audio\n",
    "            tts = gTTS(text=name, lang=language, slow=False)        \n",
    "            tts.save(path_audio)\n",
    "            \n",
    "            #cv2.imshow('image',face)\n",
    "            cv2.imwrite(path_image, face)\n",
    "            \n",
    "            #Get face encoding\n",
    "            picture = face_recognition.load_image_file(path_image)        \n",
    "            face_encoding = face_recognition.face_encodings(picture)[0]\n",
    "\n",
    "            self.update_known_faces(name, full_name, face_encoding, current_encoding)\n",
    "            return full_name\n",
    "        except Exception as e: \n",
    "            print('**s****')\n",
    "            print(e)\n",
    "            print('**s****')\n",
    "            return ''\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "import cv2\n",
    "from operator import itemgetter\n",
    "\n",
    "# Variables\n",
    "ENCODING_FREQ = 10\n",
    "encoding_count = 0\n",
    "last_faces_count = 0\n",
    "face_encodings = []\n",
    "predictions = []\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "emotion_label_window = []\n",
    "gender_label_window = []\n",
    "last_faces = []\n",
    "label_dict = {'EMOTION': '', 'GENDER': '', 'NAME': '', 'FULL_NAME': ''}\n",
    "\n",
    "# Language and localization\n",
    "AVAILABLE_LANGUAGES = ['es','en']\n",
    "LANGUAGE = 'es'\n",
    "LANGUAGE_PATH = '../lang/'\n",
    "AUDIO_PATH = '../audio/'\n",
    "IMAGE_PATH = '../images/'\n",
    "\n",
    "lang_helper = Lang_Helper(AVAILABLE_LANGUAGES, LANGUAGE_PATH, AUDIO_PATH, IMAGE_PATH, LANGUAGE)\n",
    "\n",
    "\n",
    "# Models\n",
    "model_helper = Model_Helper('../models/face/haarcascade_frontalface_default.xml', \n",
    "                            '../models/emotion/simple_CNN.530-0.65.hdf5', \n",
    "                            '../models/gender/simple_CNN.81-0.96.hdf5',\n",
    "                            AUDIO_PATH, IMAGE_PATH)\n",
    "\n",
    "\n",
    "\n",
    "# Input image \n",
    "cv2.namedWindow('main')\n",
    "video_capture = cv2.VideoCapture(0)\n",
    "\n",
    "\n",
    "while True:\n",
    "    predictions = []\n",
    "    encoding_count += 1\n",
    "    last_faces_count = len(last_faces)\n",
    "    last_faces = []\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = model_helper.face_detection.detectMultiScale(gray, 1.3, 5)\n",
    "      \n",
    "    do_encode = encoding_count>=ENCODING_FREQ or last_faces_count!=len(faces) \n",
    "    \n",
    "    if (do_encode):\n",
    "        face_encodings = []\n",
    "    \n",
    "    face_index = 0\n",
    "    \n",
    "    for (x,y,w,h) in sorted(faces, key=itemgetter(0)):\n",
    "                \n",
    "        pred_dict = label_dict.copy();\n",
    "        \n",
    "        face_index +=1 \n",
    "        face = frame[(y - y_offset):(y + h + y_offset),\n",
    "                    (x - x_offset):(x + w + x_offset)]        \n",
    "        if (do_encode):\n",
    "            print('re-encoding')\n",
    "            face_encodings.append(face_recognition.face_encodings(frame, [tuple([int(y), int(x+w), int(y+h), int(x)])])[0])\n",
    "            encoding_count = 0\n",
    "        \n",
    "        try:\n",
    "            if (len(face_encodings)>0 & face_index -1 < len(face_encodings)):\n",
    "                for i in range(len(model_helper.known_faces)):\n",
    "                    match = face_recognition.compare_faces([model_helper.known_faces[i][2]], face_encodings[face_index-1])\n",
    "                    if match[0]:\n",
    "                        pred_dict['NAME'] = model_helper.known_faces[i][0]\n",
    "                        pred_dict['FULL_NAME'] = model_helper.known_faces[i][1]\n",
    "                        break;\n",
    "                  \n",
    "        except Exception as e: \n",
    "            print('*******')\n",
    "            print(e)\n",
    "            print('*******')\n",
    "            continue            \n",
    "        #print('-----')\n",
    "        last_faces.append(cv2.cvtColor(face.copy(), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        gray_face = gray[(y - y_offset_emotion):(y + h + y_offset_emotion),\n",
    "                        (x - x_offset_emotion):(x + w + x_offset_emotion)]\n",
    "        try:\n",
    "            face = cv2.resize(face, (48, 48))\n",
    "            gray_face = cv2.resize(gray_face, (48, 48))            \n",
    "        except:\n",
    "            continue\n",
    "        face = np.expand_dims(face, 0)\n",
    "        face = preprocess_input(face)\n",
    "        gender_label_arg = np.argmax(model_helper.gender_classifier.predict(face))\n",
    "        gender = gender_labels[gender_label_arg]\n",
    "        gender_label_window.append(gender)\n",
    "\n",
    "        gray_face = preprocess_input(gray_face)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_label_arg = np.argmax(model_helper.emotion_classifier.predict(gray_face))\n",
    "        emotion = emotion_labels[emotion_label_arg]\n",
    "        emotion_label_window.append(emotion)\n",
    "\n",
    "        if len(gender_label_window) >= frame_window:\n",
    "            emotion_label_window.pop(0)\n",
    "            gender_label_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_label_window)\n",
    "            gender_mode = mode(gender_label_window)\n",
    "        except:\n",
    "            continue\n",
    "        if gender_mode == gender_labels[0]:\n",
    "            gender_color = (255, 0, 0)\n",
    "        else:\n",
    "            gender_color = (0, 255, 0)   \n",
    "        \n",
    "        pred_dict['EMOTION'] = emotion_mode\n",
    "        pred_dict['GENDER'] = gender_mode\n",
    "        \n",
    "        display_text = lang_helper.get_formatted_language_text(pred_dict)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), gender_color, 2)\n",
    "        cv2.putText(frame, display_text, (x, y - 30), font,\n",
    "                        .7, gender_color, 1, cv2.LINE_AA)\n",
    "        \n",
    "        predictions.append(pred_dict)\n",
    "\n",
    "    try:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('main', frame)        \n",
    "    except:\n",
    "        continue\n",
    "    c = chr(cv2.waitKey(2)& 255)\n",
    "    if (c!= 'ÿ'):\n",
    "        print(c + \" pressed\")   \n",
    "    command = lang_helper.get_command(c.upper())\n",
    "    if (command == 'language'):\n",
    "        print('*** Language change *** ')\n",
    "        lang_helper.switch_to_next_language()\n",
    "        lang_helper.talk('lang_change')\n",
    "    elif (command == 'who'):\n",
    "        print('*** Output predictions selected *** ')\n",
    "        if (len(predictions) > 0):\n",
    "            lang_audios =  lang_helper.get_formatted_language_audios(predictions)\n",
    "            for lang_audio in lang_audios:\n",
    "                lang_helper.play(lang_audio)\n",
    "        else:\n",
    "            lang_helper.talk('no_image')            \n",
    "    elif (command == 'save'):\n",
    "        print('*** Save person selected *** ')\n",
    "        try:\n",
    "            if (len(last_faces)==1):\n",
    "                name = '##NONE##'\n",
    "                while name == '##NONE##':\n",
    "                    lang_helper.talk('who')\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    else:\n",
    "                        name = lang_helper.capture_name() \n",
    "                        if (name=='##NONE##'):\n",
    "                            lang_helper.talk('not_understand')                            \n",
    "                        elif (name == 'cancel'):\n",
    "                            lang_helper.talk('canceled')                            \n",
    "                            break\n",
    "                        else:\n",
    "                            print('saving face...')\n",
    "                            full_name = model_helper.save_face(name, lang_helper.current_language, last_faces[0], face_encodings[face_index-1])    \n",
    "                            print('///////')\n",
    "                            print(full_name)\n",
    "                            print(lang_helper.audio_path + 'known/' + full_name + '.mp3')\n",
    "                            if (full_name!=''):\n",
    "                                lang_helper.play(lang_helper.audio_path + 'known/' + full_name + '.mp3')\n",
    "                                lang_helper.talk('saved')                        \n",
    "                            break                \n",
    "            elif (len(last_faces)>1):\n",
    "                lang_helper.talk('more_than_one_face')\n",
    "            else:\n",
    "                lang_helper.talk('no_image')\n",
    "        except:\n",
    "            continue\n",
    "    elif (command == 'quit'):\n",
    "        break\n",
    "    \n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
