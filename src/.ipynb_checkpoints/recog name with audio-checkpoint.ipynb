{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from random import *\n",
    "import os\n",
    "import glob\n",
    "\n",
    "import face_recognition\n",
    "from operator import itemgetter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "from google.cloud import speech\n",
    "import io\n",
    "import os\n",
    "\n",
    "#######################\n",
    "GOOGLE_CLOUD_SPEECH_CREDENTIALS_PATH = '../files/TFM project-287dc6d9869a.json'\n",
    "#######################\n",
    "\n",
    "def transcript_audio(filepath, language, use_cloud):\n",
    "    transcript = '##NONE##'\n",
    "    # The name of the audio file to transcribe\n",
    "    file_name = os.path.join(os.path.dirname(''), filepath)\n",
    "    \n",
    "    if use_cloud:\n",
    "        try:\n",
    "             # Instantiates a client\n",
    "            speech_client = speech.Client.from_service_account_json(GOOGLE_CLOUD_SPEECH_CREDENTIALS_PATH)\n",
    "            \n",
    "            # Loads the audio into memory\n",
    "            with io.open(file_name, 'rb') as audio_file:\n",
    "                content = audio_file.read()\n",
    "                sample = speech_client.sample(\n",
    "                    content,\n",
    "                    source_uri=None,\n",
    "                    encoding='LINEAR16',\n",
    "                    sample_rate_hertz=16000)\n",
    "\n",
    "            # Detects speech in the audio file\n",
    "            alternatives = sample.recognize(language)\n",
    "            \n",
    "            if (len(alternatives)>0):\n",
    "                transcript = alternatives[0].transcript\n",
    "        except Exception as e: \n",
    "            print(e)\n",
    "            \n",
    "    if (transcript == '##NONE##'):\n",
    "        try:\n",
    "            r = sr.Recognizer()\n",
    "            with sr.AudioFile(file_name) as source:\n",
    "                audio = r.record(source) \n",
    "            # for testing purposes, we're just using the default API key\n",
    "            # to use another API key, use `r.recognize_google(audio, key=\"GOOGLE_SPEECH_RECOGNITION_API_KEY\", show_all=True)`\n",
    "            # instead of `r.recognize_google(audio, show_all=True)`\n",
    "            alternatives = r.recognize_google(audio, show_all=False)\n",
    "            if (len(alternatives)>0):\n",
    "                transcript = alternatives\n",
    "        except sr.UnknownValueError:\n",
    "            print(\"Google Speech Recognition could not understand audio\")\n",
    "        except sr.RequestError as e:\n",
    "            print(\"Could not request results from Google Speech Recognition service; {0}\".format(e))\n",
    "       \n",
    "    return transcript\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Audio Play\n",
    "\n",
    "import pyaudio\n",
    "import wave\n",
    "import time\n",
    "import sys\n",
    "import pygame as pg\n",
    "\n",
    "def play_music(music_file, volume=0.8):\n",
    "    '''\n",
    "    stream music with mixer.music module in a blocking manner\n",
    "    this will stream the sound from disk while playing\n",
    "    '''\n",
    "    # set up the mixer\n",
    "    freq = 44100     # audio CD quality\n",
    "    bitsize = -16    # unsigned 16 bit\n",
    "    channels = 2     # 1 is mono, 2 is stereo\n",
    "    buffer = 2048    # number of samples (experiment to get best sound)\n",
    "    pg.mixer.init()\n",
    "    # volume value 0.0 to 1.0\n",
    "    pg.mixer.music.set_volume(volume)\n",
    "    clock = pg.time.Clock()\n",
    "    try:\n",
    "        pg.mixer.music.load(music_file)\n",
    "        print(\"Music file {} loaded!\".format(music_file))\n",
    "    except pg.error:\n",
    "        print(\"File {} not found! ({})\".format(music_file, pg.get_error()))\n",
    "        return\n",
    "    pg.mixer.music.play()\n",
    "    while pg.mixer.music.get_busy():\n",
    "        # check if playback has finished\n",
    "        clock.tick(30)\n",
    "        \n",
    "def play_any_audio(filename):\n",
    "    pg.mixer.init()\n",
    "    pg.mixer.music.load(filename)\n",
    "    pg.mixer.music.play()\n",
    "\n",
    "def play_audio(filename):    \n",
    "    WAVE_FILENAME = filename\n",
    "    if len(sys.argv) < 2:\n",
    "        print(\"Plays a wave file.\\n\\nUsage: %s filename.wav\" % WAVE_FILENAME)\n",
    "        sys.exit(-1)\n",
    "\n",
    "    wf = wave.open(WAVE_FILENAME, 'rb')\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    def callback(in_data, frame_count, time_info, status):\n",
    "        data = wf.readframes(frame_count)\n",
    "        return (data, pyaudio.paContinue)\n",
    "\n",
    "    stream = p.open(format=p.get_format_from_width(wf.getsampwidth()),\n",
    "                    channels=wf.getnchannels(),\n",
    "                    rate=wf.getframerate(),\n",
    "                    output=True,\n",
    "                    stream_callback=callback)\n",
    "\n",
    "    stream.start_stream()\n",
    "\n",
    "    while stream.is_active():\n",
    "        time.sleep(0.1)\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    wf.close()\n",
    "\n",
    "    p.terminate()\n",
    "    \n",
    "def record_audio(filename, seconds): \n",
    "    CHUNK = 1024\n",
    "    FORMAT = pyaudio.paInt16\n",
    "    #CHANNELS = 2\n",
    "    CHANNELS = 1\n",
    "    #RATE = 44100\n",
    "    RATE = 16000\n",
    "\n",
    "\n",
    "    p = pyaudio.PyAudio()\n",
    "\n",
    "    stream = p.open(format=FORMAT,\n",
    "                    channels=CHANNELS,\n",
    "                    rate=RATE,\n",
    "                    input=True,\n",
    "                    frames_per_buffer=CHUNK)\n",
    "\n",
    "    print(\"* recording\")\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    for i in range(0, int(RATE / CHUNK * seconds)):\n",
    "        data = stream.read(CHUNK)\n",
    "        frames.append(data)\n",
    "\n",
    "    print(\"* done recording\")\n",
    "\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    p.terminate()\n",
    "\n",
    "    wf = wave.open(filename, 'wb')\n",
    "    wf.setnchannels(CHANNELS)\n",
    "    wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "    wf.setframerate(RATE)\n",
    "    wf.writeframes(b''.join(frames))\n",
    "    wf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hombre enfadado\n"
     ]
    }
   ],
   "source": [
    "import gettext\n",
    "\n",
    "LANGUAGE_PATH = '../lang/'\n",
    "LANG = 'es'\n",
    "LANGUAGE = LANG + '/'\n",
    "AUDIO_PATH = '../audio/'\n",
    "KNOWN = 'known/'\n",
    "TEMP = 'temp/'\n",
    "IMAGE_PATH = '../images/'\n",
    "l = gettext.translation('text_', localedir=LANGUAGE_PATH, languages=[LANG])\n",
    "l.install()\n",
    "_ = l.gettext\n",
    "\n",
    "\n",
    "def get_language_audios(path, audios, preds):\n",
    "    lang_audios = []\n",
    "    for audio in audios:\n",
    "        audio_path = path + audio\n",
    "        for pred in preds:\n",
    "            audio_path = audio_path.replace('['+pred+']', preds[pred])\n",
    "        lang_audios.append(audio_path)\n",
    "    return lang_audios\n",
    "\n",
    "def get_formatted_language_audios(path, audios, predictions):\n",
    "    lang_audios = []\n",
    "    try:\n",
    "        print(path)\n",
    "        print(audios)\n",
    "        print(predictions)\n",
    "        for prediction in predictions:\n",
    "            for audio in audios:\n",
    "                key = audio.split(':')[0]\n",
    "                if (key == 'GENDER' and prediction['NAME_AUDIO'] != ''):\n",
    "                    audio_path = AUDIO_PATH + KNOWN + prediction['NAME_AUDIO']\n",
    "                    lang_audios.append(audio_path)\n",
    "                else:\n",
    "                    audio_path = path + audio.split(':')[1]                    \n",
    "                    for key in prediction:\n",
    "                        audio_path = audio_path.replace('['+key+']', prediction[key])\n",
    "                    lang_audios.append(audio_path)\n",
    "                \n",
    "\n",
    "    except Exception as e: \n",
    "        print('*a******')\n",
    "        print(e)\n",
    "        print('*a******')\n",
    "    return lang_audios\n",
    "\n",
    "def get_formatted_language_text(language, prediction):\n",
    "    lang_text = ''\n",
    "    try:\n",
    "        text_config = ''\n",
    "        with open(LANGUAGE_PATH + language + '/text_config.txt') as f:\n",
    "            for line in f:\n",
    "                text_config += line.rstrip()\n",
    "        g = text_config.split(':')[0]\n",
    "        lang_text = text_config.split(':')[1]\n",
    "        \n",
    "        for key in prediction:\n",
    "            g = g.replace('['+key+']', prediction[key])\n",
    "            \n",
    "        l = gettext.translation('text_' + g, localedir=LANGUAGE_PATH, languages=[language])\n",
    "        l.install()\n",
    "        __ = l.gettext        \n",
    "        t = ''\n",
    "        if (prediction['NAME'] != ''):  \n",
    "            t = prediction['NAME']\n",
    "        else:\n",
    "            if(prediction['GENDER'] != ''):\n",
    "                t = __(str(prediction['GENDER']))\n",
    "                \n",
    "        lang_text = lang_text.replace('[GENDER]', t) \n",
    "        t = ''\n",
    "        if(prediction['EMOTION'] != ''):\n",
    "            t = __(prediction['EMOTION'])\n",
    "            \n",
    "        lang_text = lang_text.replace('[EMOTION]', t)      \n",
    "    except Exception as e: \n",
    "        print('*t******')\n",
    "        print(e)\n",
    "        print('*t******')\n",
    "    return lang_text\n",
    "\n",
    "config_audios = []\n",
    "with open(LANGUAGE_PATH+LANGUAGE+'audio_config.txt') as f:\n",
    "    for line in f:\n",
    "        config_audios.append(line.rstrip())\n",
    "        #print(line)\n",
    "        \n",
    "\n",
    "\n",
    "label_dict = {'EMOTION': '', 'GENDER': '', 'NAME': '', 'NAME_AUDIO': ''}\n",
    "\n",
    "\n",
    "pred_test = label_dict.copy();\n",
    "pred_test['EMOTION'] = 'angry'\n",
    "pred_test['GENDER'] = 'man'\n",
    "pred_test['NAME'] = ''\n",
    "pred_test['NAME_AUDIO'] = ''\n",
    "text = get_formatted_language_text('es', pred_test)\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "AVAILABLE_LANGUAGES = ['es','en']\n",
    "\n",
    "def change_language(new_lang):\n",
    "    _config_audios = []\n",
    "    with open(LANGUAGE_PATH+new_lang+'/audio_config.txt') as f:\n",
    "        for line in f:\n",
    "            _config_audios.append(line.rstrip())\n",
    "    \n",
    "    l = gettext.translation('text_', localedir=LANGUAGE_PATH, languages=[new_lang])\n",
    "    l.install()\n",
    "    _ = l.gettext  \n",
    "    return new_lang, _config_audios, _\n",
    "            \n",
    "def switch_to_next_language():\n",
    "    idx = AVAILABLE_LANGUAGES.index(LANG)\n",
    "    new_idx = 0\n",
    "    if (idx+1 < len(AVAILABLE_LANGUAGES)):\n",
    "        new_idx = idx+1\n",
    "    return change_language(AVAILABLE_LANGUAGES[new_idx])\n",
    "    \n",
    "#LANG, config_audios, _ = switch_to_next_language()\n",
    "#print(LANG)\n",
    "#print(config_audios)\n",
    "#print(_('options'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "from statistics import mode\n",
    "from utils import preprocess_input\n",
    "from utils import get_labels\n",
    "\n",
    "# parameters\n",
    "detection_model_path = '../models/face/haarcascade_frontalface_default.xml'\n",
    "emotion_model_path = '../models/emotion/simple_CNN.530-0.65.hdf5'\n",
    "gender_model_path = '../models/gender/simple_CNN.81-0.96.hdf5'\n",
    "emotion_labels = get_labels('fer2013')\n",
    "gender_labels = get_labels('imdb')\n",
    "frame_window = 10\n",
    "x_offset_emotion = 20\n",
    "y_offset_emotion = 40\n",
    "x_offset = 30\n",
    "y_offset = 60\n",
    "\n",
    "# loading models\n",
    "face_detection = cv2.CascadeClassifier(detection_model_path)\n",
    "emotion_classifier = load_model(emotion_model_path)\n",
    "gender_classifier = load_model(gender_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexa\n",
      "Alexa-PewdgsWg.mp3\n",
      "Josu\n",
      "Josu-XPnNMiCP.mp3\n",
      "Patri\n",
      "Patri-kdUTCUch.mp3\n"
     ]
    }
   ],
   "source": [
    "known_faces = []\n",
    "\n",
    "\n",
    "for filepath in glob.iglob('../images/known/*.*', recursive=True):  \n",
    "    filename = os.path.splitext(os.path.basename(filepath))[0]+'.mp3'\n",
    "    name = os.path.splitext(filename)[0].split('-')[0]\n",
    "    picture = face_recognition.load_image_file(filepath)\n",
    "    encoding = face_recognition.face_encodings(picture)[0]\n",
    "    known_faces.append([name, filename, encoding])\n",
    "\n",
    "for i in range(len(known_faces)):\n",
    "    print(known_faces[i][0])\n",
    "    print(known_faces[i][1])\n",
    "    #print(known_faces[i][2])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gtts import gTTS\n",
    "import os\n",
    "from unidecode import unidecode\n",
    "\n",
    "def get_language_commands(available_commands):\n",
    "    l = gettext.translation('text_', localedir=LANGUAGE_PATH, languages=[LANG])\n",
    "    l.install()\n",
    "    _ = l.gettext\n",
    "    commands = []\n",
    "    for command in available_commands:\n",
    "        commands.append(_(command))\n",
    "    return commands\n",
    "\n",
    "def capture_speech():\n",
    "    rand = \"\".join(choice(string.ascii_letters) for x in range(randint(8, 8)))\n",
    "    temp_wav = AUDIO_PATH + TEMP + rand + '.wav'\n",
    "    \n",
    "    #Play beep\n",
    "    play_music(AUDIO_PATH + 'beep.mp3')\n",
    "    \n",
    "    #Record audio\n",
    "    record_audio(temp_wav, 2)\n",
    "    play_music(LANGUAGE_PATH + LANGUAGE + 'speech/one_moment.mp3')\n",
    "    \n",
    "    #Transcript audio\n",
    "    transcript = transcript_audio(temp_wav, LANG, True) \n",
    "    transcript = unidecode(transcript)\n",
    "    print('***'+transcript+'***')\n",
    "    os.remove(temp_wav)\n",
    "    return transcript.strip()\n",
    "\n",
    "def capture_selected_command():\n",
    "    available_commands = ['who','what','save','language','cancel','repeat', 'quit','options', 'keys']\n",
    "    lang_commands = get_language_commands(available_commands)\n",
    "    transcript = capture_speech()\n",
    "    if (transcript == '' or transcript == '##NONE##'): \n",
    "        return '##NONE##'\n",
    "    elif (transcript.lower() in lang_commands):\n",
    "        try:\n",
    "            return available_commands[lang_commands.index(transcript.lower())]\n",
    "        except Exception as e: \n",
    "            print('**c****')\n",
    "            print(e)\n",
    "            print('**c****')\n",
    "            return '##NONE##'\n",
    "        return transcript.lower()\n",
    "    else:\n",
    "        #return '##UNKNOWN##'\n",
    "        return '##NONE##'\n",
    "    \n",
    "\n",
    "def capture_face_and_name(face):\n",
    "    available_commands = ['cancel']\n",
    "    lang_commands = get_language_commands(available_commands)\n",
    "    \n",
    "    rand = \"\".join(choice(string.ascii_letters) for x in range(randint(8, 8)))\n",
    "    name = \"\".join(choice(string.ascii_letters) for x in range(randint(6, 6)))\n",
    "\n",
    "    transcript = capture_speech()    \n",
    "    transcript = unidecode(transcript)\n",
    "    \n",
    "    #if transcript didn't capture anything then exit \n",
    "    if (transcript == '' or transcript == '##NONE##'): \n",
    "        return None, transcript, None\n",
    "    #if transcript captures cancelation then cancel\n",
    "    elif (transcript.lower() in lang_commands):                \n",
    "        return None, available_commands[lang_commands.index(transcript.lower())], None\n",
    "    #if transcript ok then proceed\n",
    "    else:\n",
    "        mp3_name = transcript + '-' + rand + '.mp3'\n",
    "        temp_mp3 = AUDIO_PATH + KNOWN + mp3_name\n",
    "        \n",
    "        #Convert transcript to standard audio\n",
    "        tts = gTTS(text=transcript, lang=LANG, slow=False)        \n",
    "        tts.save(temp_mp3)\n",
    "\n",
    "        #Play audio back\n",
    "        play_music(temp_mp3)\n",
    "        play_music(LANGUAGE_PATH + LANGUAGE + 'speech/saved.mp3')\n",
    "\n",
    "        #Save face image\n",
    "        face_img = IMAGE_PATH + KNOWN + transcript + '-' + rand + '.jpg'\n",
    "        print(face_img)\n",
    "        #cv2.imshow('image',face)\n",
    "        cv2.imwrite(face_img, face)\n",
    "\n",
    "        #Get face encoding\n",
    "        picture = face_recognition.load_image_file(face_img)        \n",
    "        face_encoding = face_recognition.face_encodings(picture)[0]\n",
    "        print('---')\n",
    "        #print (face_encoding)\n",
    "        print (transcript)\n",
    "        print (mp3_name)\n",
    "        print('---')\n",
    "        return face_encoding, transcript, mp3_name\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_command(c):\n",
    "    command = '##NONE##'\n",
    "    if (c==' '):\n",
    "        try:           \n",
    "            while command == '##NONE##':\n",
    "                play_music(LANGUAGE_PATH + LANGUAGE + 'speech/choose_short.mp3')\n",
    "                nc = chr(cv2.waitKey(2)& 255)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                else:\n",
    "                    command = capture_selected_command()\n",
    "                    if (command=='##NONE##'):\n",
    "                        play_music(LANGUAGE_PATH + LANGUAGE + 'speech/not_understand.mp3')\n",
    "                    else:                        \n",
    "                        break                \n",
    "        except Exception as e: \n",
    "            print('*c*****')\n",
    "            print(e)\n",
    "            print('*c*****')\n",
    "    elif (c=='0'):\n",
    "        try:           \n",
    "            while command == '##NONE##':\n",
    "                play_music(LANGUAGE_PATH + LANGUAGE + 'speech/choose.mp3')\n",
    "                nc = chr(cv2.waitKey(2)& 255)\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                else:\n",
    "                    command = capture_selected_command()\n",
    "                    if (command=='##NONE##'):\n",
    "                        play_music(LANGUAGE_PATH + LANGUAGE + 'speech/not_understand.mp3')\n",
    "                    else:                        \n",
    "                        break                \n",
    "        except Exception as e: \n",
    "            print('*c*****')\n",
    "            print(e)\n",
    "            print('*c*****')\n",
    "            \n",
    "    if (c=='L' or command=='language'):\n",
    "        return 'language'\n",
    "    elif (c=='A' or command=='who'):\n",
    "        return 'who' \n",
    "    elif  (c=='S' or command=='save'):\n",
    "        return 'save'\n",
    "    elif (c=='C' or command=='cancel'):\n",
    "        return 'cancel'\n",
    "    elif (c=='R' or command=='repeat'):\n",
    "        return 'repeat'\n",
    "    if (c=='Q' or command=='quit'):\n",
    "        return 'quit'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_known_faces(known_faces, name, audio_file_name, face_encoding):\n",
    "    temp_faces = []\n",
    "    \n",
    "    # Remove previous faces with same encoding\n",
    "    for i in range(len(known_faces)):\n",
    "        match = face_recognition.compare_faces([known_faces[i][2]], face_encodings[face_index-1])\n",
    "        if match[0]:\n",
    "            print(known_faces[i][1] + ' is match')\n",
    "            image_file = IMAGE_PATH + KNOWN + os.path.splitext(os.path.basename(known_faces[i][1]))[0]+'.jpg'\n",
    "            audio_file = AUDIO_PATH + KNOWN + known_faces[i][1]\n",
    "            os.remove(image_file)\n",
    "            print(image_file + ' deleted')\n",
    "            os.remove(audio_file)\n",
    "            print(audio_file + ' deleted')\n",
    "        else:\n",
    "            print(known_faces[i][1] + ' no match')\n",
    "            temp_faces.append(known_faces[i])\n",
    "    # Add new encoding and data to known faces\n",
    "    temp_faces.append([name, audio_file_name, face_encoding])     \n",
    "    print(name + ' added')\n",
    "    \n",
    "    for i in range(len(temp_faces)):\n",
    "        print(temp_faces[i][0])\n",
    "        print(temp_faces[i][1])\n",
    "        #print(known_faces[i][2])\n",
    "\n",
    "    return temp_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "s pressed\n",
      "*** Save person selected *** \n",
      "Music file ../lang/es/speech/who.mp3 loaded!\n",
      "Music file ../audio/beep.mp3 loaded!\n",
      "* recording\n",
      "* done recording\n",
      "Music file ../lang/es/speech/one_moment.mp3 loaded!\n",
      "***Arevalo***\n",
      "Music file ../audio/known/Arevalo-TJoqlkvF.mp3 loaded!\n",
      "Music file ../lang/es/speech/saved.mp3 loaded!\n",
      "../images/known/Arevalo-TJoqlkvF.jpg\n",
      "---\n",
      "Arevalo\n",
      "Arevalo-TJoqlkvF.mp3\n",
      "---\n",
      "update known faces\n",
      "Alexa-PewdgsWg.mp3 no match\n",
      "Josu-XPnNMiCP.mp3 no match\n",
      "Patri-kdUTCUch.mp3 no match\n",
      "Peter-EdUkSQdl.mp3 no match\n",
      "Maria-mtbhjpLp.mp3 no match\n",
      "Arevalo added\n",
      "Alexa\n",
      "Alexa-PewdgsWg.mp3\n",
      "Josu\n",
      "Josu-XPnNMiCP.mp3\n",
      "Patri\n",
      "Patri-kdUTCUch.mp3\n",
      "Peter\n",
      "Peter-EdUkSQdl.mp3\n",
      "Maria\n",
      "Maria-mtbhjpLp.mp3\n",
      "Arevalo\n",
      "Arevalo-TJoqlkvF.mp3\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "re-encoding\n",
      "q pressed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# video \n",
    "video_capture = cv2.VideoCapture(0)\n",
    "font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "cv2.namedWindow('window_frame')\n",
    "emotion_label_window = []\n",
    "gender_label_window = []\n",
    "last_faces = []\n",
    "\n",
    "ENCODING_FREQ = 10\n",
    "encoding_count = 0\n",
    "last_faces_count = 0\n",
    "face_encodings = []\n",
    "predictions = []\n",
    "\n",
    "\n",
    "\n",
    "while True:\n",
    "    predictions = []\n",
    "    encoding_count += 1\n",
    "    last_faces_count = len(last_faces)\n",
    "    last_faces = []\n",
    "    _, frame = video_capture.read()\n",
    "    \n",
    "    frame_ = frame.copy()\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "    faces = face_detection.detectMultiScale(gray, 1.3, 5)\n",
    "      \n",
    "    do_encode = encoding_count>=ENCODING_FREQ or last_faces_count!=len(faces) \n",
    "    \n",
    "    if (do_encode):\n",
    "        face_encodings = []\n",
    "    \n",
    "    face_index = 0\n",
    "    \n",
    "    for (x,y,w,h) in sorted(faces, key=itemgetter(0)):\n",
    "                \n",
    "        pred_dict = label_dict.copy();\n",
    "        \n",
    "        face_index +=1 \n",
    "        face = frame[(y - y_offset):(y + h + y_offset),\n",
    "                    (x - x_offset):(x + w + x_offset)]        \n",
    "        if (do_encode):\n",
    "            print('re-encoding')\n",
    "            face_encodings.append(face_recognition.face_encodings(frame, [tuple([int(y), int(x+w), int(y+h), int(x)])])[0])\n",
    "            encoding_count = 0\n",
    "        \n",
    "        try:\n",
    "            if (len(face_encodings)>0 & face_index -1 < len(face_encodings)):\n",
    "                for i in range(len(known_faces)):\n",
    "                    match = face_recognition.compare_faces([known_faces[i][2]], face_encodings[face_index-1])\n",
    "                    if match[0]:\n",
    "                        pred_dict['NAME'] = known_faces[i][0]\n",
    "                        pred_dict['NAME_AUDIO'] = known_faces[i][1]\n",
    "                        break;\n",
    "                  \n",
    "        except Exception as e: \n",
    "            print('*******')\n",
    "            print(e)\n",
    "            print('*******')\n",
    "            continue            \n",
    "        #print('-----')\n",
    "        last_faces.append(cv2.cvtColor(face.copy(), cv2.COLOR_RGB2BGR))\n",
    "\n",
    "        gray_face = gray[(y - y_offset_emotion):(y + h + y_offset_emotion),\n",
    "                        (x - x_offset_emotion):(x + w + x_offset_emotion)]\n",
    "        try:\n",
    "            face = cv2.resize(face, (48, 48))\n",
    "            gray_face = cv2.resize(gray_face, (48, 48))            \n",
    "        except:\n",
    "            continue\n",
    "        face = np.expand_dims(face, 0)\n",
    "        face = preprocess_input(face)\n",
    "        gender_label_arg = np.argmax(gender_classifier.predict(face))\n",
    "        gender = gender_labels[gender_label_arg]\n",
    "        gender_label_window.append(gender)\n",
    "\n",
    "        gray_face = preprocess_input(gray_face)\n",
    "        gray_face = np.expand_dims(gray_face, 0)\n",
    "        gray_face = np.expand_dims(gray_face, -1)\n",
    "        emotion_label_arg = np.argmax(emotion_classifier.predict(gray_face))\n",
    "        emotion = emotion_labels[emotion_label_arg]\n",
    "        emotion_label_window.append(emotion)\n",
    "\n",
    "        if len(gender_label_window) >= frame_window:\n",
    "            emotion_label_window.pop(0)\n",
    "            gender_label_window.pop(0)\n",
    "        try:\n",
    "            emotion_mode = mode(emotion_label_window)\n",
    "            gender_mode = mode(gender_label_window)\n",
    "        except:\n",
    "            continue\n",
    "        if gender_mode == gender_labels[0]:\n",
    "            gender_color = (255, 0, 0)\n",
    "        else:\n",
    "            gender_color = (0, 255, 0)   \n",
    "        \n",
    "        pred_dict['EMOTION'] = emotion_mode\n",
    "        pred_dict['GENDER'] = gender_mode\n",
    "        \n",
    "        display_text = get_formatted_language_text(LANG, pred_dict)\n",
    "        \n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), gender_color, 2)\n",
    "        cv2.putText(frame, display_text, (x, y - 30), font,\n",
    "                        .7, gender_color, 1, cv2.LINE_AA)\n",
    "        #cv2.putText(frame, display_name, (x + 90, y - 30), font,\n",
    "        #                .7, gender_color, 1, cv2.LINE_AA)\n",
    "        \n",
    "        predictions.append(pred_dict)\n",
    "\n",
    "    try:\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        cv2.imshow('window_frame', frame)        \n",
    "    except:\n",
    "        continue\n",
    "    c = chr(cv2.waitKey(2)& 255)\n",
    "    if (c!= 'ÿ'):\n",
    "        print(c + \" pressed\")   \n",
    "    command = get_command(c.upper())\n",
    "    if (command == 'language'):\n",
    "        print('*** Language change *** ')\n",
    "        LANG, config_audios, _ = switch_to_next_language()\n",
    "        LANGUAGE = LANG + '/'\n",
    "        play_music(LANGUAGE_PATH + LANGUAGE + 'speech/lang_change.mp3')\n",
    "    elif (command == 'who'):\n",
    "        print('*** Output predictions selected *** ')\n",
    "        lang_audios =  get_formatted_language_audios(LANGUAGE_PATH + LANGUAGE, config_audios, predictions)\n",
    "        for lang_audio in lang_audios:\n",
    "            print(lang_audio)\n",
    "            play_music(lang_audio)\n",
    "    elif (command == 'save'):\n",
    "        print('*** Save person selected *** ')\n",
    "        try:\n",
    "            if (len(last_faces)==1):\n",
    "                name = '##NONE##'\n",
    "                while name == '##NONE##':\n",
    "                    play_music(LANGUAGE_PATH + LANGUAGE + 'speech/who.mp3')\n",
    "                    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                        break\n",
    "                    else:\n",
    "                        face_encoding, name, audio_file_name = capture_face_and_name(last_faces[0])                        \n",
    "                        if (name=='##NONE##'):\n",
    "                            play_music(LANGUAGE_PATH + LANGUAGE + 'speech/not_understand.mp3')\n",
    "                        elif (name == 'canceled'):\n",
    "                            play_music(LANGUAGE_PATH + LANGUAGE + 'speech/canceled.mp3')\n",
    "                            break\n",
    "                        else:\n",
    "                            print('update known faces')\n",
    "                            known_faces = update_known_faces(known_faces, name, audio_file_name, face_encoding)                                 \n",
    "                            break                \n",
    "            else:\n",
    "                play_music(LANGUAGE_PATH + LANGUAGE + 'speech/more_than_one_face.mp3')\n",
    "        except:\n",
    "            continue\n",
    "    elif (command == 'quit'):\n",
    "        break\n",
    "    \n",
    "\n",
    "video_capture.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "4\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from operator import itemgetter\n",
    "\n",
    "\n",
    "test = []\n",
    "test.append(np.array([5, 2, 3]))\n",
    "test.append(np.array([3, 2, 3]))\n",
    "test.append(np.array([4, 2, 3]))\n",
    "\n",
    "for (x,y,w) in sorted(test, key=itemgetter(0)):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alexa\n",
      "Alexa-PewdgsWg.mp3\n",
      "Josu\n",
      "Josu-XPnNMiCP.mp3\n",
      "Patri\n",
      "Patri-kdUTCUch.mp3\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(known_faces)):\n",
    "    print(known_faces[i][0])\n",
    "    print(known_faces[i][1])\n",
    "    #print(known_faces[i][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
